\documentclass[12pt]{article}
\usepackage{caption}
\usepackage{float}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage{tabto}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{tabularx}

\lstset{
    frame=single,
    breaklines=true,
    postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{red}\hookrightarrow\space}}
}

\graphicspath{{images/}}

\author{Aldar Saranov}
\date{\today}
\title{Development of an automatically configurable ant colony optimization framework. State of the art.}

\newmdenv[
  backgroundcolor=gray!20,
  frametitle=Definition,
  skipabove=\topsep,
  skipbelow=\topsep,
]{definition}

\newmdenv[
  backgroundcolor=white!20,
  frametitle=Algorithm,
  skipabove=\topsep,
  skipbelow=\topsep,
]{algorithm}

\newcommand{\dd}[1]{\mathrm{d}#1}


%--------------------------------------------------------------------------------



\begin{document}

\maketitle 
\newpage

\tableofcontents
\newpage

\begin{abstract}
Some animal species show an extreme degree of social organization. Such species (e.g. ants) have pheromone production and detection body parts and therefore seize an ability to communicate between each other in an indirect way. This concept has inspired the development of algorithms, which are based on the social behavior of the ant colonies called ant colony optimization algorithms. These algorithms allow to solve NP-hard problems in a very efficient manner. These algorithms are considered to be metaheuristics. The development of an ACO framework is the next step of formalizing this area. Such a framework can then be used as a tool to help resolving various optimization problems. This report gives a brief overview of the current state of the ACO research area, existing framework description and some tools which can be used for the automatic configuration of the framework.
\end{abstract}




\section{Introduction}

In this report we display the current research state of the Ant Colony Optimization frameworks that solve optimization problems and their configuration. To do that we split the report into several sections which will represent different practical points of the ACO application. \\
In the section 2 we introduce the basic notions of the Combinatorial Optimization Problems. These problems are described by means of two particular NP-hard problems - the  Traveling Salesman Problem and the Quadratic Assignment Problem. For them, we define the problem formulations as well as the objective functions. \\
In the section 3 we provide a description of the Ant Colony Optimization. One describes the ACO in terms of constructive heuristics. Besides, we introduce the notion of pheromone trails and explain how they affect the solution construction. In addition to the general ACO resolution algorithm, we conduct an overview of various algorithm extensions which are meant to increase its performance. Two types of pheromone update are presented - global and local. Different Ant Systems are briefly described. \\
In the section 4 we describe the application of ACO for different problem classes. Those are Continuous Optimization Problems, Multi-objective Problems, Dynamic Problems and Stochastic Problems. \\
The existing optimization frameworks are described in the section 5 where we mainly focus on single-objective optimization problem and ad hoc ACOTSPQAP framework. However, several other frameworks with different purposes and ideas are mentioned. \\
Automatic configuration process which is applied "above" the developed ACO algorithms is explained in the section 6. The top-level outline of this process is shown as well as the I-RACE implementation which is already developed and tested. \\
In section 7 we list the configurations that were rendered by the configuration process on a test set for both TSP and QAP problems. \\
The section 8 concludes the report and proposes further possible contributions into the area.

\input{section-2}
\input{section-3}
\input{section-4}





\section{Existing optimization frameworks}

\subsection{ACOTSPQAP}

ACOTSP unified framework was developed by IRIDIA group. It was developed with tendency towards the purpose and component generality. It separates the general structures of ACO metaheuristics from the problem-specific domain. All standard parameters can be specified ($\alpha, \beta, \rho, m, \epsilon, etc.$) plus one can set the specific parameters ($t_{max}, t_{min}, res_{it}$ - number of iterations since last found rb-solution, $res_{bf}$ - branching factor, $res_{dist}$ - distance between global-best and iteration-worst ants, $q_0$). All these parameters are to be tuned by an external configuration software.

\subsection{Other frameworks}

The MOACO framework was briefly described in the previous section. Besides there are some other optimization frameworks which have different features.

Multi-objective Bat Algorithm (MOBA) is based on the natural traits of bat behavior. Bat emits waves in the space which allow it to find the prey location. The bat speed, the emission  frequency and emission amplitude are the parameters which the bat is able to steer. When a bat finds a potential prey it increases the frequency and decreases the amplitude and therefore is able to locate it more precisely. In optimization problem a similar action is being done by a bat when it "flies" within the parameter search space. The general purpose of MOBA is to find the Pareto Front of the parameters \cite{moba}.

Another remarkable framework is SATenstein. It can solve single-objective optimization problems. It is based on solving satisfiability problem of a boolean formula.

Hybrid Stochastic Local Search (SLS) algorithms are based on decomposition of single-point SLS methods into separate components with generalized metaheuristic structure.


\section{Automatic configuration in IRACE}

Automatic configuration is a process that optimizes the performance of a certain algorithm as a goal function based on input parameters of the algorithm. The general parameter types are:
 
\begin{itemize}
\item \textbf{categorical} parameters - define the choice of constructive procedure, choice of branching strategies (i. e. algorithmic blocs) and so on.
\item \textbf{ordinal} parameters - define lower bounds, neighborhoods.
\item \textbf{numerical} parameters - define integer or real values/constants such as weighting factors, population sizes, temperature. They can be optional according to different categorical parameter values.
\end{itemize}

\begin{figure}[H]
  \centering
    \includegraphics[scale=0.7]{configuration-top-level.png}
  \caption{Automatic configuration scheme}
  \label{fig:autoconf}
\end{figure}

Figure \ref{fig:autoconf} shows software composition of the analyzed program and configuration script. Configuration script has parameter metadata in its disposal. Based on them, the configuration software runs the software to configure with candidate configurations one by one according to the higher-mentioned racing algorithm. The result of the run is the solution cost of cumulative runs of the algorithm on the defined problem instances. After the racing process finishes, the configuration software renders the best configuration obtained. In the most general software case there are two measures of the performance - solution quality (to maximize) and computation time (to minimize).

There are two application modes:
\begin{itemize}
\item Offline tuning - introduces a learning stage on training instances before learning on the actual set.
\item Online tuning - tunes the parameters while solving the actual instance set.
\end{itemize}

A widely used configuration algorithmic family is racing algorithms. The simplified algorithm is shown in \ref{lst:racing} and an illustration is showed in \ref{fig:irace}.


\begin{minipage}[c, breaklines=true]{0.95\textwidth}
\begin{lstlisting}[caption={General racing pseudo-code}, label={lst:racing}]
procedure racing
start with an initial candidate set Theta
repeat iterations I
	process an instance stream
	evaluate the candidates sequentially
	remove inferior candidates
until winner is selected or exit condition fulfilled
end
\end{lstlisting}
\end{minipage}

\begin{figure}[H]
  \centering
    \includegraphics[scale=1.2]{irace.jpg}
  \caption{I-RACE execution illustration}
  \label{fig:irace}
\end{figure}


I-RACE(iterated race) implementation has already been developed and applied for the ACO problem in \cite{iraceaac}. It was implemented in R with taking into account the parallel programming techniques and initial candidate set-up. The feature of the IRACE is based on iterated generation of new configurations and removing of solutions with less fitness for further evaluating on the problem instances.


Two conceptual approaches can be remarked (although their distinction is ambiguous in particular cases):
\begin{itemize}
\item Top-down - develop a fixed template-based algorithm. It is based on strictly structured algorithm which allows some parametric configuration of some of its details with minor behavior modifications.
\item Bottom-up - algorithm is build of flexible components with some rule restrictions. Often involves application of genetic programming and evolution ideas.
\end{itemize}

\section{Research state}

The higher-mentioned ACOTSP framework accompanied by IRACE R script have already produced results for the TSP and the QAP problems. The results are described in the following two subsections. General conclusion during this research is that solutions obtained by the optimal configurations are better than those obtained by the default configurations. The comparison was carried out by measuring the deviation from the optimal solution for each instance.

\subsection{Finding a better ACO configuration for the TSP}

The optimal configuration is mentioned in the table \ref{table:table-tsp}, where it was computed for different algorithms. Various instances of different sizes were generated for the configuration process.

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}
{
\begin{tabular}{|r|r|r|r|r|r|r|r|r|r|r|r|r|}
  \hline 
    algo & $m$ & $\alpha$ & $\beta$ & $\rho$ & $q_0$ & $\epsilon$ & $cl$ & $nnls$ & $ph-limits$ & $slen$ & $restart$ & $res_{it}$\\ \hline
    ACS & 28 & 3.07 & 5.09 & 0.32 & 0.53 & 0.21 & 22 & 9 & - & - & branch-factor ($res_{bf} = 1.74$) & 212\\ \hline
	MMAS & 40 & 0.94 & 4.11 & 0.72 & 0.14 & - & 18 & 12 & yes & 191 & branch-factor ($res_{bf} = 1.91$) & 367\\ \hline
\end{tabular}
}
\caption{Optimal configuration for the TSP problem.}
\label{table:table-tsp} 
\end{table} 

With local search as 3-opt + dlb-bits.

\subsection{Finding a better ACO configuration for the QAP}

Problem instances were implemented in two forms - as RR or RS. In RR the distance matrix is computed as paired euclidean distances of points distributed on a square of length 300 in uniform way. The flow matrix is generated as a matrix of uniform random values within certain range. The RS distance matrix is generated in the same way, but the flow matrix adheres to real-world flow values.

100 instances were considered where half of them was used for training and the another half for testing. The best found configuration are shown in the table \ref{table:table-qap}.

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}
{
\begin{tabular}{|r|r|r|r|r|r|r|r|r|r|r|r|}
  \hline 
    & algo & $m$ & $\alpha$ & $\rho$ & $q_0$ & $dlb-bits$ & $ph-limits$ & $slen$ & $restart$ & $res_{it}$\\ \hline
    RR & MMAS & 6 & 0.324 & 0.29 & 0.062 & yes & no & 153 & distance ($res_{bf} = 0.051$) & 22\\ \hline
	RS & MMAS & 4 & 0.164 & 0.342 & 0.284 & yes & no & 170 & branch-factor ($res_{bf} = 1.822$) & 40\\ \hline
\end{tabular}
}
\caption{Optimal configuration for the TSP problem.}
\label{table:table-qap} 
\end{table} 

With local search as 2-best-opt.

\section{Conclusions}

Vast research work has been already conducted in terms of ACO algorithms. Many types of ACO algorithms and their extensions were developed and tested. However there are still many COPs untouched. In addition, various technical details can be added in order to improve resolution performance or the development experience. 

Various ways of further area research can be proposed. The possible way is to implement new NP-hard problems resolution algorithms within ACO framework. Such practical problems as Vehicle Routning Problem, Subset Sum Problem or Knapsack Problem.

From technical point of view we can propose implementing a new framework using object-oriented paradigm which will ensure high modularity and modification-proneness. Another option is to implement the ACO algorithm stages as parallel algorithms (stages like solution generation or local search).



\begin{thebibliography}{1}


\bibitem{aco_overview} Manuel L{\'o}pez-Ib{\'a}{\~n}ez and Thomas St{\"u}tzle {\em Ant Colony Optimization: A Component-Wise Overview} 2015: IRIDIA - Technical Report Series.

\bibitem{comb_opt} Papadimitriou CH, Steiglitz K {\em Ant Colony Optimization: A Component-Wise Overview} 1982: IRIDIA - Prentice Hall, Englewood Cliffs, NJ.

\bibitem{maniezzo} Maniezzo V {\em Exact and approximate nondeterministic tree-search procedures for the quadratic assignment problem} 1999: INFORMS Journal on Computing.

\bibitem{dorigo} Dorigo M, Gambardella LM {\em Ant Colony System: A cooperative learning
approach to the traveling salesman problem} 1997: IEEE Transactions on Evolutionary
Computation.

\bibitem{lookahead} Michel R, Middendorf M {\em An island model based Ant System with
lookahead for the shortest supersequence problem} 1998: Nature, PPSN V, Lecture Notes in Computer Science.

\bibitem{candidate_list} Dorigo M, Di Caro GA {\em The Ant Colony Optimization meta-heuristic} 1999: New Ideas in Optimization, McGraw Hill, London, UK.

\bibitem{hash_table} Alba E, Chicano F {\em ACOhg: dealing with huge graphs} 2007: Proceedings of the Genetic and Evolutionary Computation Conference.

\bibitem{iterated_greedy} Ruiz R, Thomas St{\"u}tzle {\em A simple and effective iterated greedy algorithm for the permutation flowshop scheduling problem} 2007: European Journal of Operational Research.

\bibitem{iterated_ants} Wolfram Wiesemann and Thomas St{\"u}tzle {\em Iterated Ants: An Experimental Study for the Quadratic Assignment Problem} 2006: Fachbereich Wirtschaftswissenschaften.

\bibitem{cunning_ants} Shigeyoshi Tsutsui {\em cAS: Ant Colony Optimization with Cunning
Ants} 2006: Hannan University, Matsubara Osaka.

\bibitem{mmas} Thomas St{\"u}tzle and Hoos HH {\em MAXâ€“MIN Ant System} 2000: Future Generation Computer Systems.

\bibitem{ras} Bullnheimer B, Hartl R, Strauss C {\em A new rank-based version of the
Ant System} 1999: Central European Journal for Operations Research and Economics.

\bibitem{eas} Dorigo M {\em Optimization, learning and natural algorithms} 1992: Dipartimento di Elettronica, Politecnico di Milano, Italy.

\bibitem{bwas} Cordon O, de Viana IF, Herrera F, Moreno L {\em A new ACO model
integrating evolutionary computation concepts: The best-worst ant system} 2000: Second International Workshop on Ant Algorithms.

\bibitem{coop_tsp} Dorigo M, Gambardella LM {\em Ant Colony System: A cooperative learning
approach to the traveling salesman problem.} 1997: IEEE Transactions on Evolutionary
Computation.

\bibitem{iraceaac} Manuel L{\'o}pez-Ib{\'a}{\~n}ez  and  J{\'e}r{\'e}mie Dubois-Lacoste  and Leslie {P{\'e}rez C{\'a}ceres}  and  Thomas St{\"u}tzle  and  Mauro Birattari {\em Iterated Racing for Automatic Algorithm Configuration} 2013: IRIDIA - Technical Report Series.

\bibitem{aco_continuous} Socha K, Dorigo M {\em Iterated Racing for Automatic Algorithm Configuration} 2008: European Journal of Operational Research.
  
\bibitem{aco_incremental} Liao T, Montes de Oca MA, Aydin D, St{\"u}tzle T, Dorigo M {\em An incremental ant colony algorithm with local search for continuous optimization} 2011: Proceedings of the Genetic and Evolutionary
Computation Conference.  
  
\bibitem{moaco} L{\'o}pez-Ib{\'a}{\~n}ez M, St{\"u}tzle T {\em The automatic design of multi-objective ant colony optimization algorithms} 2012: IEEE Transactions on Evolutionary Computation.    
  
\bibitem{moba} Xin-She Yang {\em Bat Algorithm for Multi-objective Optimisation} 2011: Department of Engineering, University of Cambridge.  
 
 
\bibitem{external_memory} Dorigo, Marco
and Birattari, Mauro
and Blum, Christian
and Gambardella, Luca Maria
and Mondada, Francesco
and St{\"u}tzle, Thomas {\em IAn External Memory Implementation in Ant Colony Optimization} 2004: Springer Berlin Heidelberg - Proceedings.

\end{thebibliography}

 
\end{document} 
